# Regret Matching for Discrete Colonel Blotto (S=5, K=3) — Hart Comparison

This repository contains a small, reproducible experiment suite for **two-player zero-sum discrete Colonel Blotto**
and a **sampled regret-matching** solver in the style of **Neller & Lanctot** (realised opponent action update).

The core goal is to:
1. Run regret matching self-play to obtain approximate equilibrium behaviour.
2. Measure convergence via **exploitability** and **value**.
3. Compare the learned strategy (after symmetrisation) to the **Hart target random-battlefield marginal** for the specific case \((S,K)=(5,3)\).

---

## Repo layout

At the top level:

- `Code/`  
  All source code for the experiments (game construction, regret matching, evaluation, plotting).

- `Resources/`  
  Papers / notes / references used for the project (e.g. Hart, Neller–Lanctot, course notes).  

Inside `Code/`:

- `game.py`  
  Builds the Blotto instance: enumerates actions (allocations) and constructs the payoff matrix `A`.

- `rm.py`  
  **Regret Matching (sampled)** self-play:
  - each time step uses mixed strategies from positive regrets,
  - samples realised actions,
  - updates regrets using the realised opponent move.

- `sym.py`  
  Builds permutation classes of allocations (treats battlefield labels as exchangeable).

- `metrics.py`  
  Evaluation utilities:
  - exploitability / Nash gap,
  - value of mixed strategies,
  - induced random-battlefield marginal,
  - TV distance,
  - Hart target marginal for \((S,K)=(5,3)\).

- `run.py`  
  Main experiment script. Produces figures and prints report-ready summary statistics.

- `figures/`  
  Output directory (generated by `run.py`).

---

## Setup

### 1) Create/activate a virtual environment (recommended)

```bash
python -m venv venv
source venv/bin/activate  # macOS/Linux
# .\venv\Scripts\activate # Windows PowerShell
```
### 2) Install dependencies
```bash
pip install -r requirement.txt
```

## Quickstart: run the main experiment

Default run (S=5, K=3, T=50000, 20 seeds for multi-seed diagnostics):

```bash
python run.py
```

Shorter run
```bash
python run.py --T 5000
```

Increase the number of seeds used for the multi-seed plots:
```bash
python run.py --T 5000 --nseeds 50
```

Use an explicit list of seeds: 
```bash
python run.py --T 5000 --seeds 0 1 2 3 4 5
```

Change main seed used for the single-run time-series curves:
```bash
python run.py --seed_main 7
```

## Expected outputs

### Terminal summary

`run.py` prints a compact summary:

- **Main run (final T)**
  - exploitability (avg strategies)
  - exploitability (symmetrised avg strategies)
  - value of avg strategies
  - TV distance to Hart marginal
- **Across seeds (if enabled)**
  - exploitability median + IQR + range
  - TV median + IQR + range
- **Rescaled tail constants (if enabled)**
  - median/IQR of `√T · ε(p̄_T, q̄_T)`
  - median/IQR of `T · ε(p̄_T^sym, q̄_T^sym)`

### Figures (saved to `figures/`)

#### Single-seed plots

1. `exploitability_vs_T.png`  
   Exploitability vs time for:
   - averaged strategies `(p̄_T, q̄_T)`
   - symmetrised averaged strategies `(p̄_T^sym, q̄_T^sym)`

2. `value_vs_T.png`  
   Value `p̄_T^T A q̄_T` across time  
   (should approach the game value; for symmetric zero-sum instances here it is near 0).

3. `tv_vs_T.png`  
   TV distance between the learned symmetrised random-battlefield marginal and the Hart target marginal (for `(S,K)=(5,3)`).

4. `marginal_bar.png`  
   Side-by-side bar chart at final `T`: learned marginal vs Hart target marginal.

#### Multi-seed plots

5. `tv_vs_exploit_scatter.png`  
   Scatter across seeds:
   - x-axis: exploitability of symmetrised averages
   - y-axis: TV distance to Hart marginal

6. `exploitability_rescaled_band.png`  
   Multi-seed median ± IQR band of rescaled diagnostics:
   - `√T · ε(p̄_T, q̄_T)` (expected “flat-ish” if `ε = O(1/√T)`)
   - `T · ε(p̄_T^sym, q̄_T^sym)` (often flatter if symmetrisation improves rate empirically)


